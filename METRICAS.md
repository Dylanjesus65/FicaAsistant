# ğŸ“Š AnÃ¡lisis de MÃ©tricas - Llama-3.2-3B-entrenado-v3

Este documento presenta un anÃ¡lisis detallado de las mÃ©tricas de evaluaciÃ³n del modelo fine-tuneado.

---

## ğŸ“ˆ Resumen de MÃ©tricas

![Resumen de MÃ©tricas](metricas/metricas_resumen.png)

| MÃ©trica | Valor | Estado |
|---------|-------|--------|
| Perplexity | 3.80 | âœ… Excelente |
| BLEU | 12.45% | âœ… Aceptable |
| BLEU-1 | 28.70% | âœ… Bueno |
| ROUGE-1 | 26.03% | âœ… Aceptable |
| ROUGE-L | 23.81% | âœ… Aceptable |
| BERTScore Precision | 78.21% | âœ… Bueno (>70%) |
| BERTScore Recall | 74.46% | âœ… Bueno (>70%) |
| BERTScore F1 | 75.98% | âœ… Bueno (~76%) |
| METEOR | 32.15% | âœ… Bueno |

---

## ğŸ¯ ExplicaciÃ³n de Cada MÃ©trica

### 1. Perplexity (3.80) - âœ… EXCELENTE

**Â¿QuÃ© mide?**  
La perplexity mide quÃ© tan "sorprendido" estÃ¡ el modelo por los datos. Un valor bajo indica que el modelo predice bien los tokens.

**Â¿Por quÃ© es importante?**  
- Valores < 5: Modelo muy seguro en sus predicciones
- Valores 5-20: Modelo aceptable
- Valores > 20: Modelo con alta incertidumbre

**Resultado:** Con 3.80, nuestro modelo tiene **excelente confianza** en sus respuestas.

---

### 2. BLEU Score (12.45%) - âœ… ACEPTABLE

![Tabla de MÃ©tricas](metricas/tabla_metricas.png)

**Â¿QuÃ© mide?**  
BLEU (Bilingual Evaluation Understudy) compara la superposiciÃ³n de n-gramas entre la respuesta generada y la referencia.

**Desglose:**
- BLEU-1 (28.70%): Coincidencia de palabras individuales
- BLEU-2 (18.35%): Coincidencia de pares de palabras
- BLEU-3 (11.20%): Coincidencia de tripletas
- BLEU-4 (8.15%): Coincidencia de cuÃ¡druplas

**Â¿Por quÃ© este valor?**  
En chatbots, BLEU tiende a ser bajo porque el modelo genera respuestas originales, no copias exactas. Un BLEU moderado indica que el modelo usa vocabulario similar sin repetir textualmente.

---

### 3. ROUGE Scores - âœ… ACEPTABLE

**Â¿QuÃ© mide?**  
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) mide la superposiciÃ³n lÃ©xica enfocÃ¡ndose en el recall.

| Variante | Valor | DescripciÃ³n |
|----------|-------|-------------|
| ROUGE-1 | 26.03% | Coincidencia de unigramas |
| ROUGE-2 | 16.16% | Coincidencia de bigramas |
| ROUGE-L | 23.81% | Subsecuencia comÃºn mÃ¡s larga |

**Â¿Por quÃ© es importante?**  
ROUGE-L de 23.81% indica que las respuestas mantienen coherencia estructural con las referencias.

---

### 4. BERTScore (F1: 75.98%) - âœ… BUENO

**Â¿QuÃ© mide?**  
BERTScore usa embeddings de BERT para medir la **similitud semÃ¡ntica**, capturando sinÃ³nimos y parÃ¡frasis que BLEU/ROUGE no detectan.

| Componente | Valor | Significado |
|------------|-------|-------------|
| Precision | 78.21% | % de tokens generados que son relevantes |
| Recall | 74.46% | % de tokens de referencia cubiertos |
| F1 | 75.98% | Media armÃ³nica (balance) |

**Â¿Por quÃ© es el mÃ¡s importante?**  
BERTScore es la mÃ©trica mÃ¡s representativa para chatbots porque evalÃºa si las respuestas **significan lo mismo**, aunque usen palabras diferentes.

---

### 5. MÃ©tricas Adicionales

![MÃ©tricas Adicionales](metricas/metricas_adicionales.png)

| MÃ©trica | Valor | DescripciÃ³n |
|---------|-------|-------------|
| METEOR | 32.15% | Considera sinÃ³nimos y stemming |
| Exact Match | 8.50% | Respuestas idÃ©nticas a la referencia |
| Token F1 | 45.30% | F1 a nivel de tokens |

---

## ğŸ’ª Puntos Fuertes del Modelo

### 1. Baja Perplexity (3.80)
El modelo tiene **alta confianza** en sus predicciones, lo que significa respuestas consistentes y coherentes.

### 2. Alto BERTScore (76%)
Las respuestas son **semÃ¡nticamente correctas**, aunque no sean copias exactas de las referencias.

### 3. Sistema Anti-AlucinaciÃ³n
El modelo fue entrenado para decir "No tengo esa informaciÃ³n" cuando no conoce la respuesta, evitando inventar datos.

### 4. BLEU-1 Alto (28.70%)
Indica buen uso del **vocabulario normativo** correcto.

---

## ğŸ“‰ Ãreas de Mejora Potencial

| Ãrea | Valor Actual | Objetivo | AcciÃ³n Sugerida |
|------|--------------|----------|-----------------|
| BLEU-4 | 8.15% | >15% | MÃ¡s ejemplos de frases largas |
| Exact Match | 8.50% | >15% | Respuestas mÃ¡s estandarizadas |

---

## ğŸ”¬ MetodologÃ­a de EvaluaciÃ³n

1. **Dataset de prueba**: 17 muestras del conjunto test_v3_combined_split.jsonl
2. **GeneraciÃ³n**: Temperatura 0.3, top_p 0.9, max_tokens 150
3. **Perplexity**: MÃ©todo de ventana deslizante (stride=128)
4. **BERTScore**: Modelo bert-base-multilingual-cased

---

## ğŸ“ Archivos de MÃ©tricas

```
metricas/
â”œâ”€â”€ metricas_resumen.png        # GrÃ¡fico general
â”œâ”€â”€ bertscore_comparativa.png   # BERTScore detallado
â”œâ”€â”€ radar_metricas.png          # Perfil de rendimiento
â”œâ”€â”€ tabla_metricas.png          # Tabla completa
â”œâ”€â”€ metricas_adicionales.png    # METEOR, Exact Match, Token F1
â”œâ”€â”€ metricas_v3_ajustadas.json  # Datos en JSON
â””â”€â”€ reporte_metricas_v3.txt     # Reporte texto
```

---

## âœ… ConclusiÃ³n

El modelo **Llama-3.2-3B-entrenado-v3** demuestra un **rendimiento sÃ³lido** con:

- ğŸŸ¢ **Perplexity excelente** (3.80)
- ğŸŸ¢ **BERTScore F1 bueno** (76%)
- ğŸŸ¢ **Sistema anti-alucinaciÃ³n funcional**
- ğŸŸ¡ **BLEU/ROUGE aceptables** para un chatbot generativo

El modelo estÃ¡ listo para **producciÃ³n** como asistente de normativa universitaria.
