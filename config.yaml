model_name: "meta-llama/Meta-Llama-3-8B"
new_model_name: "llama-3-8b-finetuned"

# LoRA parameters
lora_r: 64
lora_alpha: 16
lora_dropout: 0.1

# 4-bit quantization parameters
use_4bit: true
bnb_4bit_compute_dtype: "float16"
bnb_4bit_quant_type: "nf4"
use_nested_quant: false

# Training arguments
output_dir: "./results"
num_train_epochs: 1
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
learning_rate: 2.0e-4
weight_decay: 0.001
optim: "paged_adamw_32bit"
lr_scheduler_type: "cosine"
max_grad_norm: 0.3
warmup_ratio: 0.03
group_by_length: true
save_steps: 25
logging_steps: 25
max_seq_length: 512
packing: false
device_map: "auto"
dataset_name: "d:\\FineTuning\\dataset" # Dataset local generado
dataset_text_field: "text" # Campo generado por dataset_builder.py
